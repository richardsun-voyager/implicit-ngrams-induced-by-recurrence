{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gX6rKfD8CyNH"
   },
   "source": [
    "## Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1021,
     "status": "ok",
     "timestamp": 1596873868610,
     "user": {
      "displayName": "Richard Sun",
      "photoUrl": "",
      "userId": "07723018862498440951"
     },
     "user_tz": -480
    },
    "id": "1-AwwneWTY-F",
    "outputId": "1410042b-546c-4fe3-8cbd-2a7e904e7de1"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5379,
     "status": "ok",
     "timestamp": 1597469728944,
     "user": {
      "displayName": "Richard Sun",
      "photoUrl": "",
      "userId": "07723018862498440951"
     },
     "user_tz": -480
    },
    "id": "ejldGJAGCyNJ",
    "outputId": "0cd2e0db-307a-4b92-b274-81b902190009"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import argparse\n",
    "from helper import data_generator\n",
    "import sys; sys.argv=['']; del sys\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "file = 'args/semeval2010_8.yaml'\n",
    "with open(file) as f:\n",
    "    args = yaml.load(f, Loader=yaml.Loader)\n",
    "    parser = argparse.ArgumentParser(description='attention')\n",
    "    config = parser.parse_args()\n",
    "    for k, v in args['common'].items():\n",
    "        setattr(config, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(config.dic_path, 'rb') as f:\n",
    "    vocab, word2id, id2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dg = data_generator(config, config.train_path)\n",
    "#train_eval_dg = data_generator(config, config.train_path, False)\n",
    "dev_dg = data_generator(config, config.dev_path, False)\n",
    "test_dg = data_generator(config, config.test_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(item[0]) for item in train_dg.data_batch]\n",
    "max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2065,
     "status": "ok",
     "timestamp": 1597469731628,
     "user": {
      "displayName": "Richard Sun",
      "photoUrl": "",
      "userId": "07723018862498440951"
     },
     "user_tz": -480
    },
    "id": "DsDOEdpkCyNc"
   },
   "outputs": [],
   "source": [
    "from torch.nn import utils as nn_utils\n",
    "import torch\n",
    "from torch import optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from self_defined_lstm_linear import LSTM\n",
    "from self_defined_gru_linear import GRU\n",
    "from self_defined_rnn_linear import RNN\n",
    "\n",
    "# from dynamic_lstm import dynamicLSTM\n",
    "# from dynamic_gru import dynamicGRU\n",
    "# from dynamic_rnn import dynamicRNN\n",
    "# from ngram_rnn import nRNN\n",
    "# from matrix_multi_model import MatMultiModel\n",
    "\n",
    "# from self_defined_lstm  import LSTM\n",
    "# from self_defined_gru  import GRU\n",
    "# from self_defined_rnn  import RNN\n",
    "\n",
    "# from self_defined_lstm_no_combination import LSTM\n",
    "# from self_defined_gru_no_combination import GRU\n",
    "# from self_defined_rnn_no_combination import RNN\n",
    "#from self_defined_simple_recurrence2 import RNN as sRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1597469734506,
     "user": {
      "displayName": "Richard Sun",
      "photoUrl": "",
      "userId": "07723018862498440951"
     },
     "user_tz": -480
    },
    "id": "9fTKpWOECyNf"
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, label_dim=1, rnn_type='lstm'):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        #self.embeddings.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.label_dim = label_dim\n",
    "\n",
    "        self.affine = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.cnn = nn.Conv1d(embed_dim, hidden_dim, 3, 1, padding=2)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         if rnn_type == 'elman':\n",
    "#             self.rnn = dynamicRNN(hidden_dim, hidden_dim//2, \n",
    "#                           bidirectional=True)\n",
    "#             self.encoder = dynamicRNN(hidden_dim, hidden_dim, \n",
    "#                           bidirectional=False)\n",
    "#         elif rnn_type == 'gru':\n",
    "#             self.rnn = dynamicGRU(embed_dim, hidden_dim//2, \n",
    "#                           bidirectional=True)\n",
    "#             self.encoder = dynamicGRU(hidden_dim, hidden_dim, \n",
    "#                           bidirectional=False)\n",
    "#         else:\n",
    "#             self.rnn = dynamicLSTM(embed_dim, hidden_dim, \n",
    "#                           bidirectional=True)\n",
    "#             self.encoder = dynamicLSTM(hidden_dim, hidden_dim, \n",
    "#                           bidirectional=False)\n",
    "        if rnn_type == 'elman':\n",
    "            self.rnn = RNN(embed_dim, hidden_dim, \n",
    "                          bidirectional=False)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = GRU(embed_dim, hidden_dim, \n",
    "                          bidirectional=False)\n",
    "        elif rnn_type == 'nrnn':\n",
    "            self.rnn = nRNN(embed_dim, hidden_dim, \n",
    "                          bidirectional=False)\n",
    "        elif rnn_type == 'srnn':\n",
    "            self.rnn = sRNN(embed_dim, hidden_dim, \n",
    "                          bidirectional=False)\n",
    "        elif rnn_type == 'mrnn':\n",
    "            self.rnn = MatMultiModel(int(np.sqrt(embed_dim)), hidden_dim, \n",
    "                      bidirectional=False)\n",
    "        else:\n",
    "            self.rnn = LSTM(embed_dim, hidden_dim, \n",
    "                          bidirectional=False)\n",
    "    \n",
    "        self.linear = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "        self.decoder = nn.Linear(hidden_dim, label_dim, bias=False)\n",
    "\n",
    "    # batch_size * sent_l * dim\n",
    "    def forward(self, seq_ids, seq_lengths=None):\n",
    "        '''\n",
    "        Args:\n",
    "            seq_ids: word indexes, batch_size, max_len, Long Tensor\n",
    "            seq_lengths: lengths of sentences, batch_size, Long Tensor\n",
    "        attention:\n",
    "            score = v h\n",
    "            att = softmax(score)\n",
    "        '''\n",
    "        \n",
    "        seq_embs = self.embeddings(seq_ids)\n",
    "        seq_embs = self.dropout(seq_embs)\n",
    "        #print(seq_embs.shape)\n",
    "        batch_size, max_len, hidden_dim = seq_embs.size()\n",
    "        # batch * max_len * hidden_states\n",
    "        #hidden_vecs = self.affine(seq_embs)\n",
    "        #hidden_vecs = self.cnn(seq_embs.transpose(1,2))\n",
    "        #hidden_vecs = hidden_vecs.transpose(1,2)\n",
    "        #print(hidden_vecs.shape)\n",
    "        hidden_vecs, final_vec = self.rnn(seq_embs, seq_lengths)\n",
    "        #_, final_vec = self.encoder(hidden_vecs, seq_lengths)\n",
    "        final_vec = self.dropout(final_vec)\n",
    "        senti_scores = self.decoder(final_vec)\n",
    "        #multi class\n",
    "        if self.label_dim == 1:\n",
    "            probs = self.sigmoid(senti_scores)\n",
    "        else:\n",
    "            probs = self.softmax(senti_scores)\n",
    "            logits = torch.log(probs + 0.000000000001)\n",
    "            return logits, senti_scores\n",
    "        return probs, senti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, label_dim=1, bw=1, be=0.1, bm=0.1, dropout=0.5):\n",
    "        '''\n",
    "        Average the GRU hidden vectors\n",
    "        '''\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embeddings.weight.data.uniform_(-be, be)\n",
    "        self.cnn = nn.Conv1d(embed_dim, hidden_dim, kernel_size=20, padding=0, stride=20,bias=False)\n",
    "        #self.cnn = nn.Conv1d(embed_dim, hidden_dim, kernel_size=3, padding=2, stride=1)\n",
    "        #self.cnn.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.affine = nn.Linear(embed_dim, hidden_dim)\n",
    "        #self.affine.weight.data.uniform_(-bm, bm)\n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_dim, label_dim)\n",
    "        #self.decoder.weight.data.uniform_(-bw, bw)\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        assert label_dim > 0\n",
    "        self.label_dim = label_dim\n",
    "\n",
    "    # batch_size * sent_l * dim\n",
    "    def forward(self, seq_ids, seq_lengths=None):\n",
    "        '''\n",
    "        Args:\n",
    "            seq_ids: word indexes, batch_size, max_len, Long Tensor\n",
    "            seq_lengths: lengths of sentences, batch_size, Long Tensor\n",
    "        attention:\n",
    "            score = v tanh(Wh+b)\n",
    "            att = softmax(score)\n",
    "        '''\n",
    "        batch_size, max_len = seq_ids.size()\n",
    "        seq_embs = self.embeddings(seq_ids)\n",
    "        seq_embs = self.dropout(seq_embs)\n",
    "        # batch * max_len * hidden_states\n",
    "        #hidden_vecs = self.affine(seq_embs)\n",
    "        #hidden_vecs = seq_embs\n",
    "        #batch size, emb_dim, sent len\n",
    "        seq_embs = seq_embs.transpose(1, 2)\n",
    "        #batch size, out_dim, sent_len -1\n",
    "        hidden_vecs = self.cnn(seq_embs)\n",
    "\n",
    "\n",
    "        \n",
    "        h = []\n",
    "        for i in range(batch_size):\n",
    "            #h_i, _ = torch.tanh(hidden_vecs[i, :, :seq_lengths[i]]).max(1)\n",
    "            #h_i = torch.tanh(hidden_vecs[i, :, :seq_lengths[i]]).mean(1)\n",
    "            #h_i, _ = hidden_vecs[i, :, :seq_lengths[i]].max(1)\n",
    "            #h_i = hidden_vecs[i, :, :seq_lengths[i]].mean(1)\n",
    "            h_i = hidden_vecs[i, :, :seq_lengths[i]].sum(1)\n",
    "            h.append(h_i)\n",
    "        final_vec = torch.stack(h)\n",
    "\n",
    "#         final_vec = torch.bmm(attn, hidden_vecs).squeeze(1)\n",
    "        final_vec = self.dropout(final_vec)\n",
    "        scores = self.decoder(final_vec)\n",
    "        if self.label_dim == 1:\n",
    "            probs = self.sigmoid(scores)\n",
    "        else:\n",
    "            probs = self.softmax(scores)\n",
    "            logits = torch.log(probs + 0.000000000001)\n",
    "            return logits, scores\n",
    "        return probs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1597469738705,
     "user": {
      "displayName": "Richard Sun",
      "photoUrl": "",
      "userId": "07723018862498440951"
     },
     "user_tz": -480
    },
    "id": "4mu6fXJWCyNk"
   },
   "outputs": [],
   "source": [
    "##Evaluation classification\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "def evaluate_cls(dg, model, label_dim=1):\n",
    "    #Make prediction\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    #record the gold and the prediction\n",
    "    gold_labels = []\n",
    "    pred_labels = []\n",
    "    dg.reset_samples()\n",
    "    while dg.index<dg.data_len:\n",
    "        sent_ids, label_list, sent_lens = next(dg.get_ids_samples())\n",
    "        outputs, _ =  model(sent_ids.to(device), sent_lens.to(device))\n",
    "        if label_dim == 1:\n",
    "            preds = (outputs>0.5).squeeze()\n",
    "            num = (preds.cpu() == label_list.bool()).sum().cpu().item()\n",
    "        else:\n",
    "            preds = outputs.argmax(1)\n",
    "            num = (label_list==preds.cpu()).sum().item()\n",
    "        gold_labels += list(label_list.cpu().numpy())\n",
    "        pred_labels += list(preds.cpu().numpy())\n",
    "        count += num\n",
    "\n",
    "    accuracy = count*1.0/dg.data_len\n",
    "    f1 = f1_score(gold_labels, pred_labels, average='macro')\n",
    "    print('Evaluation accuracy:', accuracy)\n",
    "    return accuracy, f1\n",
    "\n",
    "#regression\n",
    "def evaluate_reg(dg, model):\n",
    "    #Make prediction\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    #record the gold and the prediction\n",
    "    error = 0\n",
    "    dg.reset_samples()\n",
    "    while dg.index<dg.data_len:\n",
    "        sent_ids, label_list, sent_lens = next(dg.get_ids_samples())\n",
    "        probs, scores = model(sent_ids.to(device), sent_lens.to(device))\n",
    "        #label_list.apply_(scale_value)\n",
    "        loss = loss_func(scores.squeeze(), label_list.float().to(device))\n",
    "        num = len(sent_lens)\n",
    "        error += loss.item() * num\n",
    "        count += num\n",
    "\n",
    "    mse = error/count\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_loss(scores, label_list):\n",
    "    pred_labels = scores.argmax(1) \n",
    "    pred_scores = torch.gather(scores, 1, pred_labels.unsqueeze(0))\n",
    "    pred_scores = pred_scores.squeeze(0)\n",
    "    gold_scores = torch.gather(scores, 1, label_list.to(device).unsqueeze(0))\n",
    "    gold_scores = gold_scores.squeeze(0)\n",
    "    left = torch.log(1+ torch.exp(2*(2.5 - gold_scores)))\n",
    "    right = torch.log(1+ torch.exp(2*(0.5 + pred_scores)))\n",
    "    return left+right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55364,
     "status": "error",
     "timestamp": 1597469809620,
     "user": {
      "displayName": "Richard Sun",
      "photoUrl": "",
      "userId": "07723018862498440951"
     },
     "user_tz": -480
    },
    "id": "STPSEMhcCyNo",
    "outputId": "cf30a75b-52c9-4c61-d28e-03c636fd7d95",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVMA-L\n",
      "Epoch: 0\n",
      "####################\n",
      "Sample Loss:2.104\n",
      "Evaluation accuracy: 0.299\n",
      "f1: 0.1198\n",
      "Epoch: 1\n",
      "####################\n",
      "Evaluation accuracy: 0.405\n",
      "f1: 0.261\n",
      "Epoch: 2\n",
      "####################\n",
      "Sample Loss:1.820\n",
      "Evaluation accuracy: 0.437\n",
      "f1: 0.3006\n",
      "Epoch: 3\n",
      "####################\n",
      "Evaluation accuracy: 0.486\n",
      "f1: 0.3788\n",
      "Epoch: 4\n",
      "####################\n",
      "Sample Loss:1.447\n",
      "Evaluation accuracy: 0.513\n",
      "f1: 0.4138\n",
      "Epoch: 5\n",
      "####################\n",
      "Evaluation accuracy: 0.519\n",
      "f1: 0.419\n",
      "Epoch: 6\n",
      "####################\n",
      "Sample Loss:1.238\n",
      "Evaluation accuracy: 0.537\n",
      "f1: 0.4546\n",
      "Epoch: 7\n",
      "####################\n",
      "Evaluation accuracy: 0.54\n",
      "f1: 0.4578\n",
      "Epoch: 8\n",
      "####################\n",
      "Sample Loss:1.121\n",
      "Evaluation accuracy: 0.531\n",
      "f1: 0.4378\n",
      "Epoch: 9\n",
      "####################\n",
      "Evaluation accuracy: 0.566\n",
      "f1: 0.4863\n",
      "Epoch: 10\n",
      "####################\n",
      "Sample Loss:0.941\n",
      "Evaluation accuracy: 0.554\n",
      "f1: 0.4792\n",
      "Epoch: 11\n",
      "####################\n",
      "Evaluation accuracy: 0.591\n",
      "f1: 0.5144\n",
      "Epoch: 12\n",
      "####################\n",
      "Sample Loss:1.051\n",
      "Evaluation accuracy: 0.595\n",
      "f1: 0.5241\n",
      "Epoch: 13\n",
      "####################\n",
      "Evaluation accuracy: 0.594\n",
      "f1: 0.535\n",
      "Epoch: 14\n",
      "####################\n",
      "Sample Loss:0.891\n",
      "Evaluation accuracy: 0.608\n",
      "f1: 0.5478\n",
      "Epoch: 15\n",
      "####################\n",
      "Evaluation accuracy: 0.603\n",
      "f1: 0.539\n",
      "Epoch: 16\n",
      "####################\n",
      "Sample Loss:0.723\n",
      "Evaluation accuracy: 0.58\n",
      "f1: 0.512\n",
      "Epoch: 17\n",
      "####################\n",
      "Evaluation accuracy: 0.603\n",
      "f1: 0.5468\n",
      "Epoch: 18\n",
      "####################\n",
      "Sample Loss:1.106\n",
      "Evaluation accuracy: 0.603\n",
      "f1: 0.5507\n",
      "Epoch: 19\n",
      "####################\n",
      "Evaluation accuracy: 0.607\n",
      "f1: 0.5578\n",
      "Epoch: 20\n",
      "####################\n",
      "Sample Loss:0.870\n",
      "Evaluation accuracy: 0.609\n",
      "f1: 0.561\n",
      "Epoch: 21\n",
      "####################\n",
      "Evaluation accuracy: 0.617\n",
      "f1: 0.5627\n",
      "Epoch: 22\n",
      "####################\n",
      "Sample Loss:0.601\n",
      "Evaluation accuracy: 0.605\n",
      "f1: 0.5508\n",
      "Epoch: 23\n",
      "####################\n",
      "Evaluation accuracy: 0.599\n",
      "f1: 0.5469\n",
      "Epoch: 24\n",
      "####################\n",
      "Sample Loss:0.600\n",
      "Evaluation accuracy: 0.585\n",
      "f1: 0.5455\n",
      "Epoch: 25\n",
      "####################\n",
      "Evaluation accuracy: 0.613\n",
      "f1: 0.5694\n",
      "Epoch: 26\n",
      "####################\n",
      "Sample Loss:0.630\n",
      "Evaluation accuracy: 0.602\n",
      "f1: 0.5489\n",
      "Epoch: 27\n",
      "####################\n",
      "Evaluation accuracy: 0.603\n",
      "f1: 0.5673\n",
      "Epoch: 28\n",
      "####################\n",
      "Sample Loss:0.399\n",
      "Evaluation accuracy: 0.602\n",
      "f1: 0.551\n",
      "Epoch: 29\n",
      "####################\n",
      "Evaluation accuracy: 0.596\n",
      "f1: 0.5476\n",
      "Epoch: 30\n",
      "####################\n",
      "Sample Loss:0.721\n",
      "Evaluation accuracy: 0.597\n",
      "f1: 0.5533\n",
      "Epoch: 31\n",
      "####################\n",
      "Evaluation accuracy: 0.6\n",
      "f1: 0.5524\n",
      "Epoch: 32\n",
      "####################\n",
      "Sample Loss:0.435\n",
      "Evaluation accuracy: 0.6\n",
      "f1: 0.5505\n",
      "Epoch: 33\n",
      "####################\n",
      "Evaluation accuracy: 0.602\n",
      "f1: 0.5647\n",
      "Epoch: 34\n",
      "####################\n",
      "Sample Loss:0.498\n",
      "Evaluation accuracy: 0.6\n",
      "f1: 0.5553\n",
      "Epoch: 35\n",
      "####################\n",
      "Evaluation accuracy: 0.611\n",
      "f1: 0.5596\n",
      "Epoch: 36\n",
      "####################\n",
      "Sample Loss:0.280\n",
      "Evaluation accuracy: 0.608\n",
      "f1: 0.5813\n",
      "Epoch: 37\n",
      "####################\n",
      "Evaluation accuracy: 0.622\n",
      "f1: 0.5766\n",
      "Epoch: 38\n",
      "####################\n",
      "Sample Loss:0.323\n",
      "Evaluation accuracy: 0.617\n",
      "f1: 0.5716\n",
      "Epoch: 39\n",
      "####################\n",
      "Evaluation accuracy: 0.6\n",
      "f1: 0.5599\n",
      "Best valid f1: 0.5813\n",
      "Evaluation accuracy: 0.6359955833640044\n",
      "test f1: 0.5681559194273121\n",
      "MVMA-L\n",
      "Epoch: 0\n",
      "####################\n",
      "Sample Loss:2.470\n",
      "Evaluation accuracy: 0.265\n",
      "f1: 0.093\n",
      "Epoch: 1\n",
      "####################\n",
      "Evaluation accuracy: 0.38\n",
      "f1: 0.2168\n",
      "Epoch: 2\n",
      "####################\n",
      "Sample Loss:1.764\n",
      "Evaluation accuracy: 0.422\n",
      "f1: 0.2924\n",
      "Epoch: 3\n",
      "####################\n",
      "Evaluation accuracy: 0.443\n",
      "f1: 0.3186\n",
      "Epoch: 4\n",
      "####################\n",
      "Sample Loss:1.653\n",
      "Evaluation accuracy: 0.514\n",
      "f1: 0.407\n",
      "Epoch: 5\n",
      "####################\n",
      "Evaluation accuracy: 0.532\n",
      "f1: 0.4331\n",
      "Epoch: 6\n",
      "####################\n",
      "Sample Loss:1.621\n",
      "Evaluation accuracy: 0.541\n",
      "f1: 0.4513\n",
      "Epoch: 7\n",
      "####################\n",
      "Evaluation accuracy: 0.548\n",
      "f1: 0.4645\n",
      "Epoch: 8\n",
      "####################\n",
      "Sample Loss:1.249\n",
      "Evaluation accuracy: 0.566\n",
      "f1: 0.4951\n",
      "Epoch: 9\n",
      "####################\n",
      "Evaluation accuracy: 0.559\n",
      "f1: 0.4833\n",
      "Epoch: 10\n",
      "####################\n",
      "Sample Loss:0.960\n",
      "Evaluation accuracy: 0.586\n",
      "f1: 0.5106\n",
      "Epoch: 11\n",
      "####################\n",
      "Evaluation accuracy: 0.589\n",
      "f1: 0.5256\n",
      "Epoch: 12\n",
      "####################\n",
      "Sample Loss:0.887\n",
      "Evaluation accuracy: 0.605\n",
      "f1: 0.5342\n",
      "Epoch: 13\n",
      "####################\n",
      "Evaluation accuracy: 0.588\n",
      "f1: 0.529\n",
      "Epoch: 14\n",
      "####################\n",
      "Sample Loss:0.622\n",
      "Evaluation accuracy: 0.601\n",
      "f1: 0.5372\n",
      "Epoch: 15\n",
      "####################\n",
      "Evaluation accuracy: 0.606\n",
      "f1: 0.5463\n",
      "Epoch: 16\n",
      "####################\n",
      "Sample Loss:0.897\n",
      "Evaluation accuracy: 0.602\n",
      "f1: 0.5512\n",
      "Epoch: 17\n",
      "####################\n",
      "Evaluation accuracy: 0.602\n",
      "f1: 0.5487\n",
      "Epoch: 18\n",
      "####################\n",
      "Sample Loss:0.769\n",
      "Evaluation accuracy: 0.608\n",
      "f1: 0.5586\n",
      "Epoch: 19\n",
      "####################\n",
      "Evaluation accuracy: 0.613\n",
      "f1: 0.5642\n",
      "Epoch: 20\n",
      "####################\n",
      "Sample Loss:0.506\n",
      "Evaluation accuracy: 0.612\n",
      "f1: 0.5557\n",
      "Epoch: 21\n",
      "####################\n",
      "Evaluation accuracy: 0.609\n",
      "f1: 0.5625\n",
      "Epoch: 22\n",
      "####################\n",
      "Sample Loss:0.521\n",
      "Evaluation accuracy: 0.607\n",
      "f1: 0.5536\n",
      "Epoch: 23\n",
      "####################\n",
      "Evaluation accuracy: 0.583\n",
      "f1: 0.5464\n",
      "Epoch: 24\n",
      "####################\n",
      "Sample Loss:0.439\n",
      "Evaluation accuracy: 0.607\n",
      "f1: 0.5709\n",
      "Epoch: 25\n",
      "####################\n",
      "Evaluation accuracy: 0.609\n",
      "f1: 0.5665\n",
      "Epoch: 26\n",
      "####################\n",
      "Sample Loss:0.452\n",
      "Evaluation accuracy: 0.609\n",
      "f1: 0.5664\n",
      "Epoch: 27\n",
      "####################\n",
      "Evaluation accuracy: 0.595\n",
      "f1: 0.5603\n",
      "Epoch: 28\n",
      "####################\n",
      "Sample Loss:0.400\n",
      "Evaluation accuracy: 0.607\n",
      "f1: 0.5717\n",
      "Epoch: 29\n",
      "####################\n",
      "Evaluation accuracy: 0.601\n",
      "f1: 0.5726\n",
      "Epoch: 30\n",
      "####################\n",
      "Sample Loss:0.551\n",
      "Evaluation accuracy: 0.609\n",
      "f1: 0.5787\n",
      "Epoch: 31\n",
      "####################\n",
      "Evaluation accuracy: 0.616\n",
      "f1: 0.5806\n",
      "Epoch: 32\n",
      "####################\n",
      "Sample Loss:0.488\n",
      "Evaluation accuracy: 0.623\n",
      "f1: 0.576\n",
      "Epoch: 33\n",
      "####################\n",
      "Evaluation accuracy: 0.617\n",
      "f1: 0.5764\n",
      "Epoch: 34\n",
      "####################\n",
      "Sample Loss:0.525\n",
      "Evaluation accuracy: 0.62\n",
      "f1: 0.5773\n",
      "Epoch: 35\n",
      "####################\n",
      "Evaluation accuracy: 0.612\n",
      "f1: 0.5777\n",
      "Epoch: 36\n",
      "####################\n",
      "Sample Loss:0.363\n",
      "Evaluation accuracy: 0.611\n",
      "f1: 0.5757\n",
      "Epoch: 37\n",
      "####################\n",
      "Evaluation accuracy: 0.601\n",
      "f1: 0.557\n",
      "Epoch: 38\n",
      "####################\n",
      "Sample Loss:0.284\n",
      "Evaluation accuracy: 0.617\n",
      "f1: 0.5802\n",
      "Epoch: 39\n",
      "####################\n",
      "Evaluation accuracy: 0.612\n",
      "f1: 0.5778\n",
      "Best valid f1: 0.5806\n",
      "Evaluation accuracy: 0.6308428413691571\n",
      "test f1: 0.5630688319827295\n",
      "MVMA-L\n",
      "Epoch: 0\n",
      "####################\n",
      "Sample Loss:2.213\n",
      "Evaluation accuracy: 0.332\n",
      "f1: 0.1724\n",
      "Epoch: 1\n",
      "####################\n",
      "Evaluation accuracy: 0.381\n",
      "f1: 0.244\n",
      "Epoch: 2\n",
      "####################\n",
      "Sample Loss:1.750\n",
      "Evaluation accuracy: 0.434\n",
      "f1: 0.3187\n",
      "Epoch: 3\n",
      "####################\n",
      "Evaluation accuracy: 0.447\n",
      "f1: 0.3343\n",
      "Epoch: 4\n",
      "####################\n",
      "Sample Loss:1.687\n",
      "Evaluation accuracy: 0.521\n",
      "f1: 0.4272\n",
      "Epoch: 5\n",
      "####################\n",
      "Evaluation accuracy: 0.527\n",
      "f1: 0.4557\n",
      "Epoch: 6\n",
      "####################\n",
      "Sample Loss:1.108\n",
      "Evaluation accuracy: 0.521\n",
      "f1: 0.4463\n",
      "Epoch: 7\n",
      "####################\n",
      "Evaluation accuracy: 0.539\n",
      "f1: 0.4591\n",
      "Epoch: 8\n",
      "####################\n",
      "Sample Loss:1.167\n",
      "Evaluation accuracy: 0.541\n",
      "f1: 0.4695\n",
      "Epoch: 9\n",
      "####################\n",
      "Evaluation accuracy: 0.569\n",
      "f1: 0.5024\n",
      "Epoch: 10\n",
      "####################\n",
      "Sample Loss:1.081\n",
      "Evaluation accuracy: 0.565\n",
      "f1: 0.4904\n",
      "Epoch: 11\n",
      "####################\n",
      "Evaluation accuracy: 0.572\n",
      "f1: 0.5\n",
      "Epoch: 12\n",
      "####################\n",
      "Sample Loss:0.871\n",
      "Evaluation accuracy: 0.566\n",
      "f1: 0.4975\n",
      "Epoch: 13\n",
      "####################\n",
      "Evaluation accuracy: 0.582\n",
      "f1: 0.527\n",
      "Epoch: 14\n",
      "####################\n",
      "Sample Loss:0.740\n",
      "Evaluation accuracy: 0.586\n",
      "f1: 0.5306\n",
      "Epoch: 15\n",
      "####################\n",
      "Evaluation accuracy: 0.59\n",
      "f1: 0.5398\n",
      "Epoch: 16\n",
      "####################\n",
      "Sample Loss:0.516\n",
      "Evaluation accuracy: 0.584\n",
      "f1: 0.5235\n",
      "Epoch: 17\n",
      "####################\n",
      "Evaluation accuracy: 0.601\n",
      "f1: 0.5514\n",
      "Epoch: 18\n",
      "####################\n",
      "Sample Loss:0.699\n",
      "Evaluation accuracy: 0.596\n",
      "f1: 0.5379\n",
      "Epoch: 19\n",
      "####################\n",
      "Evaluation accuracy: 0.611\n",
      "f1: 0.5656\n",
      "Epoch: 20\n",
      "####################\n",
      "Sample Loss:0.745\n",
      "Evaluation accuracy: 0.602\n",
      "f1: 0.5538\n",
      "Epoch: 21\n",
      "####################\n",
      "Evaluation accuracy: 0.598\n",
      "f1: 0.5478\n",
      "Epoch: 22\n",
      "####################\n",
      "Sample Loss:0.552\n",
      "Evaluation accuracy: 0.6\n",
      "f1: 0.5553\n",
      "Epoch: 23\n",
      "####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.596\n",
      "f1: 0.5578\n",
      "Epoch: 24\n",
      "####################\n",
      "Sample Loss:0.597\n",
      "Evaluation accuracy: 0.599\n",
      "f1: 0.5545\n",
      "Epoch: 25\n",
      "####################\n",
      "Evaluation accuracy: 0.604\n",
      "f1: 0.5686\n",
      "Epoch: 26\n",
      "####################\n",
      "Sample Loss:0.538\n",
      "Evaluation accuracy: 0.613\n",
      "f1: 0.5768\n",
      "Epoch: 27\n",
      "####################\n",
      "Evaluation accuracy: 0.604\n",
      "f1: 0.5684\n",
      "Epoch: 28\n",
      "####################\n",
      "Sample Loss:0.547\n",
      "Evaluation accuracy: 0.597\n",
      "f1: 0.5583\n",
      "Epoch: 29\n",
      "####################\n",
      "Evaluation accuracy: 0.608\n",
      "f1: 0.5753\n",
      "Epoch: 30\n",
      "####################\n",
      "Sample Loss:0.415\n",
      "Evaluation accuracy: 0.604\n",
      "f1: 0.5667\n",
      "Epoch: 31\n",
      "####################\n",
      "Evaluation accuracy: 0.591\n",
      "f1: 0.5574\n",
      "Epoch: 32\n",
      "####################\n",
      "Sample Loss:0.323\n",
      "Evaluation accuracy: 0.614\n",
      "f1: 0.5766\n",
      "Epoch: 33\n",
      "####################\n",
      "Evaluation accuracy: 0.615\n",
      "f1: 0.5715\n",
      "Epoch: 34\n",
      "####################\n",
      "Sample Loss:0.401\n",
      "Evaluation accuracy: 0.61\n",
      "f1: 0.572\n",
      "Epoch: 35\n",
      "####################\n",
      "Evaluation accuracy: 0.61\n",
      "f1: 0.591\n",
      "Epoch: 36\n",
      "####################\n",
      "Sample Loss:0.429\n",
      "Evaluation accuracy: 0.609\n",
      "f1: 0.5845\n",
      "Epoch: 37\n",
      "####################\n",
      "Evaluation accuracy: 0.611\n",
      "f1: 0.587\n",
      "Epoch: 38\n",
      "####################\n",
      "Sample Loss:0.345\n",
      "Evaluation accuracy: 0.61\n",
      "f1: 0.5816\n",
      "Epoch: 39\n",
      "####################\n",
      "Evaluation accuracy: 0.614\n",
      "f1: 0.562\n",
      "Best valid f1: 0.591\n",
      "Evaluation accuracy: 0.6175929333824071\n",
      "test f1: 0.563005288445044\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(555)\n",
    "# np.random.seed(555)\n",
    "label_dim = 19\n",
    "loss_func = nn.BCELoss()\n",
    "if label_dim>1:\n",
    "    loss_func = nn.NLLLoss()\n",
    "\n",
    "valid_f1_list = []\n",
    "test_f1_list = []\n",
    "for scale in range(3):\n",
    "    #torch.manual_seed(666)\n",
    "    #model = SimpleAttnClassifier(config.vocab_size, 100, 100, 1, scale)\n",
    "    hidden_dim = 300\n",
    "    model = SimpleClassifier(config.vocab_size, 300, hidden_dim, label_dim, 'lstm')\n",
    "    #model = SimpleClassifier(config.vocab_size, 32*32, 32, label_dim, 'mrnn')\n",
    "    #model = CNNClassifier(config.vocab_size, 300, 300, label_dim)\n",
    "    #model.load_vector(config.emb_path, trainable=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    ##################################\n",
    "    ####Weight decay can influence the result, if the value is too large, the model will not converge after iterations\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.5, weight_decay=0.000000)\n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=0.5, weight_decay=0.00000, lr_decay=0.001)\n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.000001, lr_decay=0.001)\n",
    "\n",
    "    \n",
    "    loop_num = int(train_dg.data_len/config.batch_size)+1\n",
    "    best_model = None\n",
    "    best_acc = -1\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    for i in range(40):\n",
    "        print('Epoch:', i)\n",
    "        print('#'*20)\n",
    "        \n",
    "        ##For gru\n",
    "        #optimizer = optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.00001, lr_decay=0.001)\n",
    "        #for lstm\n",
    "        #optimizer = optim.Adagrad(model.parameters(), lr=0.02, weight_decay=0.00002, lr_decay=0.0001)\n",
    "        #for elman\n",
    "        #optimizer = optim.Adagrad(model.parameters(), lr=0.02, weight_decay=0.00002, lr_decay=0.0001)\n",
    "        #for elman\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        #shuffle the training set given the random seed\n",
    "        train_dg.shuffle_data()\n",
    "        #sequential sampling, use all the dataset\n",
    "        train_dg.reset_samples()\n",
    "        #model.embeddings.required_grad = config.update_emb\n",
    "        for j in range(loop_num):\n",
    "            model.zero_grad()\n",
    "            # generate dataset\n",
    "            sent_ids,  label_list, sent_lens = next(train_dg.get_sequential_ids_samples())\n",
    "            logits, scores = model(sent_ids.to(device), sent_lens.to(device))\n",
    "            #loss = ranking_loss(scores, label_list).mean()\n",
    "            #label_list.apply_(scale_value)\n",
    "            if label_dim == 1:\n",
    "                loss = loss_func(logits.squeeze(), label_list.float().to(device))#\n",
    "            else:\n",
    "                loss = loss_func(logits.squeeze(), label_list.to(device))\n",
    "            \n",
    "            # Do the backward pass and update the gradient\n",
    "            #w_h = model.rnn.f_cell.weight_ih\n",
    "            #loss += w_h.norm(2)**2*0.0001#lstm\n",
    "            #loss += w_h.norm(2)**2*0.01#gru\n",
    "#             if i>5:\n",
    "#                 loss += w_h.norm(2)**2*0.01\n",
    "            loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(),0.25)#0.05\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "            optimizer.step()\n",
    "        if i%2== 0:\n",
    "            l = loss.cpu().item()\n",
    "            print('Sample Loss:{:.3f}'.format(l))\n",
    "\n",
    "        #Dev Evaluation\n",
    "        #print('Training Accuracy')\n",
    "        #t_acc = evaluate(train_eval_dg, model)\n",
    "        #print('Dev Accuracy')\n",
    "        dev_acc, f1 = evaluate_cls(dev_dg, model, label_dim)\n",
    "        print('f1:', round(f1, 4))\n",
    "        #train_acc.append(t_acc)\n",
    "        valid_acc.append(dev_acc)\n",
    "        if best_acc < f1:\n",
    "            best_acc = f1\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "        #best_model = model\n",
    "#     #Test performance\n",
    "#     print('Training Accuracy')\n",
    "#     t_acc = evaluate(train_eval_dg, model)\n",
    "    #best_model = copy.copy(model)\n",
    "    #best_model.load_state_dict(best_model_dict)\n",
    "    #del best_model_dict\n",
    "    print('Best valid f1:', round(best_acc,4))\n",
    "    _, f1 = evaluate_cls(test_dg, best_model, label_dim)\n",
    "    #Test performance       \n",
    "    print('test f1:', f1)\n",
    "    valid_f1_list.append(best_acc)\n",
    "    test_f1_list.append(f1)\n",
    "#     score_list = []\n",
    "#     for neg_phrase in negation_phrases_filtered:\n",
    "#         v = get_phrase_polarity(neg_phrase, best_model, hidden_dim)\n",
    "#         score_list.append(v)\n",
    "#     neg_phrase_scores.append(score_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, t in zip(valid_f1_list, test_f1_list):\n",
    "    print(round(v*100, 1), round(t*100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract n-grams for each specific relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elman_ngram_feature import *\n",
    "from gru_ngram_feature import *\n",
    "# from lstm_ngram_feature import *\n",
    "# from all_ngram_feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Message-Topic(e1,e2)': 0,\n",
    " 'Product-Producer(e2,e1)': 1,\n",
    " 'Instrument-Agency(e2,e1)': 2,\n",
    " 'Entity-Destination(e1,e2)': 3,\n",
    " 'Cause-Effect(e2,e1)': 4,\n",
    " 'Component-Whole(e1,e2)': 5,\n",
    " 'Product-Producer(e1,e2)': 6,\n",
    " 'Member-Collection(e2,e1)': 7,\n",
    " 'Other': 8,\n",
    " 'Entity-Origin(e1,e2)': 9,\n",
    " 'Content-Container(e1,e2)': 10,\n",
    " 'Entity-Origin(e2,e1)': 11,\n",
    " 'Cause-Effect(e1,e2)': 12,\n",
    " 'Component-Whole(e2,e1)': 13,\n",
    " 'Content-Container(e2,e1)': 14,\n",
    " 'Instrument-Agency(e1,e2)': 15,\n",
    " 'Message-Topic(e2,e1)': 16,\n",
    " 'Member-Collection(e1,e2)': 17,\n",
    " 'Entity-Destination(e2,e1)': 18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e1> cigarettes </e1> are used 2.0632264614105225\n",
      "cigarettes </e1> are used by 0.006082646548748016\n",
      "</e1> are used by <e2> 4.044539928436279\n",
      "are used by <e2> women 2.1829018592834473\n",
      "used by <e2> women </e2> 0.6761854290962219\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sent = '<e1> demolition </e1> was the cause of <e2> terror </e2>'.split()\n",
    "sent = '<e1> damage </e1> caused by the <e2> bombing </e2>'.split()\n",
    "sent = '<e1> courtyard </e1> of the <e2> castle </e2>'.split()\n",
    "sent = '<e1> marble </e1> was dropped into the <e2> bowl </e2>'.split()\n",
    "sent = '<e1> car </e1> left the <e2> plant </e2>'.split()\n",
    "sent = '<e1> cigarettes </e1> by the major <e2> producer </e2>'.split()\n",
    "sent = '<e1> cigarettes </e1> are used by <e2> women </e2>'.split()\n",
    "for ngram in ngrams(sent, 5):\n",
    "    ngram_id = [word2id[w] for w in ngram]\n",
    "    score = get_phrase_polarity_multi(ngram_id, best_model, 300)\n",
    "    print(' '.join(ngram), score[12].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_phrase_polarity = get_phrase_polarity_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RuXWeGUICyO2",
    "ixc1hrcbCyP2",
    "x4iARfG-CyP8"
   ],
   "name": "rnn_approx_classification_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
